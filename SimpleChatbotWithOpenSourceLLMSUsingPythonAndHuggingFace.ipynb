{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPbtzTfKopEzgN8XXQPrE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhayPrasad25/AI-ML-Projectarium/blob/main/SimpleChatbotWithOpenSourceLLMSUsingPythonAndHuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing Requirements"
      ],
      "metadata": {
        "id": "qJNKOwNmmnFb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcok0uvkgo7G",
        "outputId": "444485ed-eb98-464b-daf2-c6f1fa29ae69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import our required tools from the transformers library"
      ],
      "metadata": {
        "id": "f2dmmYtBmsbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "Z_Ba74CHhGaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting the model"
      ],
      "metadata": {
        "id": "PqRt1WiDm3cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/blenderbot-400M-distill\""
      ],
      "metadata": {
        "id": "hFZLR2GPmZct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch the model and intialize a Tokenizer"
      ],
      "metadata": {
        "id": "HgoQLvTenGDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model (download on first run and reference local installation for consequent runs)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "45OuBeKAnKsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat"
      ],
      "metadata": {
        "id": "VvMG9cqCnk7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1. keeping track of the conversation History"
      ],
      "metadata": {
        "id": "Po4EZCFonym1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []"
      ],
      "metadata": {
        "id": "Whw9gOOannqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Encoding the Conversation History"
      ],
      "metadata": {
        "id": "8590SKNPoEpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_string = \"/n\".join(conversation_history)\n",
        "history_string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tGxKqJ9aoRgf",
        "outputId": "68361fd0-e456-4be7-9350-e49e882b8527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Fetch Prompt from the user"
      ],
      "metadata": {
        "id": "W-8Xdnpeova8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text =  \"hello ? How are you doing\"\n",
        "input_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "04Lojh-uo2xo",
        "outputId": "48bbd668-5b2b-42da-d69e-422c1a07c9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello ? How are you doing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4. Tokenization of the User Prompt History"
      ],
      "metadata": {
        "id": "r_kqtxiGpCxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode_plus(history_string, input_text, return_tensors = \"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOqyWzfUpVFu",
        "outputId": "a15c8b9e-5db4-48ef-b847-76d11e593723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[1710,   86, 2453,  855,  366,  304,  929]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helps in Referencing the content"
      ],
      "metadata": {
        "id": "MeIh8S1v4Nit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pretrained_vocab_files_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE7MM7EgpuyM",
        "outputId": "0e1aab58-d725-4838-a73f-8a612bc9cf2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5. Generate Output from model"
      ],
      "metadata": {
        "id": "C1HWi6Hz4U4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cFFIu5b4jLz",
        "outputId": "bfa24f33-73a6-42e6-fa23-5ce965181f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  1, 281, 476, 929, 731,  21, 281, 632, 929, 731,  19, 544, 366, 304,\n",
              "          38, 281, 632, 368, 929, 731,   2]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding the output"
      ],
      "metadata": {
        "id": "q3whg-5o4w4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(outputs[0],skip_special_tokens = True).strip()\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZgtW63MH40tD",
        "outputId": "1d6ab3da-4f4c-461a-9c8d-b2b963813b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm doing well. I am doing well, how are you? I am not doing well\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating the Conversation History"
      ],
      "metadata": {
        "id": "akVJ3qNl5DrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history.append(input_text)\n",
        "conversation_history.append(response)\n",
        "conversation_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxw8E6Ez5JKU",
        "outputId": "bb1578d5-cbbc-4375-addb-b184fb442b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello ? How are you doing',\n",
              " \"I'm doing well. I am doing well, how are you? I am not doing well\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat (made the whole function together)"
      ],
      "metadata": {
        "id": "KEEv4w5m5YMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  # Create conversation history string\n",
        "  history_string = \"/n\".join(conversation_history)\n",
        "\n",
        "  # Get the input data from the user\n",
        "  input_text = input(\"> \")\n",
        "\n",
        "  # Tokenize the input text and hsitory\n",
        "  inputs = tokenizer.encode_plus(history_string, input_text, return_tensors = \"pt\")\n",
        "\n",
        "  # Genearte the response from the model\n",
        "  outputs = model.generate(**inputs)\n",
        "\n",
        "  # Decode the response\n",
        "  response = tokenizer.decode(outputs[0],skip_special_tokens = True).strip()\n",
        "\n",
        "  # Print the response\n",
        "  print(response)\n",
        "\n",
        "  #Add interaction to the conversation history\n",
        "  conversation_history.append(input_text)\n",
        "  conversation_history.append(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJjIhZTR5bAg",
        "outputId": "fcc9e2e1-bc0c-4f41-887f-51bda06ff5d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry to hear that. I hope it gets better for you. I'm doing okay.\n",
            "I don't have any problems, thank you for asking. What do you do for a living?\n",
            "I work in a warehouse. It pays the bills, but it's not what I want to do.\n"
          ]
        }
      ]
    }
  ]
}